---
title: "Decision Trees and Ensembles: From Splits to Random Forests and XGBoost"
description: "A complete guide to decision trees, covering entropy, information gain, one-hot encoding, regression trees, and ensemble methods like Random Forest and XGBoost."
pubDate: "2024-04-03"
tags: ["dev", "machine-learning", "decision-trees", "xgboost"]
author: "Jongmin Lee"
heroImage: "/4-Decision-trees/Screenshot_2024-04-03_at_5.44.16_PM.png"
draft: false
---

# 4. Decision trees

# Decision trees

![Screenshot 2024-04-03 at 5.44.16 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_5.44.16_PM.png)

![Screenshot 2024-04-03 at 5.44.50 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_5.44.50_PM.png)

![Screenshot 2024-04-03 at 5.45.20 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_5.45.20_PM.png)

## Learning Process

![Screenshot 2024-04-03 at 5.49.39 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_5.49.39_PM.png)

![Screenshot 2024-04-03 at 5.51.26 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_5.51.26_PM.png)

![Screenshot 2024-04-03 at 5.57.17 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_5.57.17_PM.png)

# Decision tree learning

## Measuring purity

![Screenshot 2024-04-03 at 7.47.45 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.47.45_PM.png)

![Screenshot 2024-04-03 at 7.48.46 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.48.46_PM.png)

## Choosing a split: Information Gain

![Screenshot 2024-04-03 at 7.50.16 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.50.16_PM.png)

![Screenshot 2024-04-03 at 7.50.27 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.50.27_PM.png)

## Putting it together

![Screenshot 2024-04-03 at 7.52.21 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.52.21_PM.png)

![Screenshot 2024-04-03 at 7.52.38 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.52.38_PM.png)

## Using one-hot encoding of categorical features

![Screenshot 2024-04-03 at 7.55.55 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.55.55_PM.png)

![Screenshot 2024-04-03 at 7.56.04 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.56.04_PM.png)

![Screenshot 2024-04-03 at 7.56.29 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.56.29_PM.png)

![Screenshot 2024-04-03 at 7.56.37 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.56.37_PM.png)

## Continuous valued features

![Screenshot 2024-04-03 at 7.58.06 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.58.06_PM.png)

![Screenshot 2024-04-03 at 7.58.19 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.58.19_PM.png)

![Screenshot 2024-04-03 at 7.58.28 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.58.28_PM.png)

## Regression Trees

![Screenshot 2024-04-03 at 7.59.28 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.59.28_PM.png)

![Screenshot 2024-04-03 at 7.59.43 PM.png](/4-Decision-trees/Screenshot_2024-04-03_at_7.59.43_PM.png)

# Tree ensembles

## Using multiple decision trees

![Screenshot 2024-04-06 at 11.07.46 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.07.46_AM.png)

![Screenshot 2024-04-06 at 11.08.06 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.08.06_AM.png)

## Sampling with replacement

![Screenshot 2024-04-06 at 11.09.42 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.09.42_AM.png)

![Screenshot 2024-04-06 at 11.09.54 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.09.54_AM.png)

## Random forest algorithm

![Screenshot 2024-04-06 at 11.11.49 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.11.49_AM.png)

![Screenshot 2024-04-06 at 11.12.00 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.12.00_AM.png)

## XGBoost

![Screenshot 2024-04-06 at 11.14.27 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.14.27_AM.png)

![Screenshot 2024-04-06 at 11.14.37 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.14.37_AM.png)

![Screenshot 2024-04-06 at 11.15.06 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.15.06_AM.png)

## When to use decision trees

![Screenshot 2024-04-06 at 11.19.05 AM.png](/4-Decision-trees/Screenshot_2024-04-06_at_11.19.05_AM.png)